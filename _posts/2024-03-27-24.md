---
title: "3장 선형(Linear) 회귀분석의 가설과 비용함수"
categories: "ML"
---

<h2>3-1</h2>
![image](https://github.com/ymin1108/ymin1108.github.io/assets/71661158/69ea1dae-9a1a-4b1b-8efb-3dc1e84dc196)<br/>
- 첫 번째 코드는 TensorFlow 라이브러리를 불러오는 코드입니다.
- 두 번째 줄은 TensorFlow의 난수 생성기에 시드 값을 설정하는 코드입니다.<br/><br/>

![image](https://github.com/ymin1108/ymin1108.github.io/assets/71661158/2df1de92-dc88-4b3f-aa8a-60dfbbd88a1d)<br/>
- x_train, y_train에 각각 [1,2,3]이라는 데이터를 저장합니다.
- x_train은 입력 데이터이고, y_train은 해당 입력 데이터에 대한 실제 값입니다.
- `W = tf.Variable(tf.random.normal([1]), name='weight')` `b = tf.Variable(tf.random.normal([1]), name='bias')` 이 두 부분은 가중치(W)와 편향(b)을 초기화하는 부분입니다.
- `tf.random.normal([1])`는 평균이 0이고 표준 편차가 1인 정규 분포에서 무작위 값을 생성합니다.
- 위 사진에서는 0.35265976입니다.
- `hypothesis=x_train*W`는 가설 함수를 설정하는 부분입니다.
- `error=tf.reduce_mean(tf.square(hypothesis - y_train))` 이 부분은 손실 함수를 설정하는 부분입니다. 여기서는 가설 함수와 실제 값의 차이의 제곱의 평균을 계산하여 손실 함수를 구현했습니다.<br/><br/>

![image](https://github.com/ymin1108/ymin1108.github.io/assets/71661158/2e04eb75-3e00-4541-96a7-5bc6d51af836)<br/>
- `hypothesis = x_train * W + b` 이 부분은 가설 함수를 설정하는 부분입니다. 여기서는 입력 데이터에 가중치를 곱하고 편향을 더하여 가설 함수를 구현했습니다.
- `error= tf.reduce_mean(tf.square(hypothesis - y_train))` 이 부분은 손실 함수를 설정하는 부분입니다. 여기서는 가설 함수와 실제 값의 차이의 제곱의 평균을 계산하여 손실 함수를 구현했습니다.<br/><br/>

![image](https://github.com/ymin1108/ymin1108.github.io/assets/71661158/19eb0737-2206-4b59-89fc-d7ac4e08f927)<br/>
- `optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)` 이 코드는 경사하강법을 사용하여 객체를 생성합니다. `learning_rate=0.01`은 학습률을 설정하는 부분으로 학습률은 모델의 매개변수를 조절하는 비율을 나타냅니다.
- 3번째 줄은 손실 함수를 최소화하는 매개변수 W와 b를 업데이트합니다.
- 마지막 코드는 200번째 단계마다 학습 단계, 손실 함수의 값, 매개변수들을 출력합니다.<br/><br/>

<h2>3-2</h2>
![image](https://github.com/ymin1108/ymin1108.github.io/assets/71661158/85102dde-0e25-472a-84ac-c331b77068ab)<br/>
- x1은 평균이 0.0이고 표준편차가 0.5인 정규 분포에서 랜덤하게 생성된 값입니다.
- y1은 x1에 대한 선형 함수에 약간의 노이즈를 추가한 값입니다.
- 이렇게 생성된 데이터를 sample_data 리스트에 추가합니다.
- sample_data에서 x1과 y1을 각각 추출하여 x_data, y_data로 분리합니다.
- 그리고 x_data, y_data를 빨간색 점으로 표시합니다.<br/><br/>

![image](https://github.com/ymin1108/ymin1108.github.io/assets/71661158/8634ab8e-b7e1-4f9c-88e5-849884f03ac7)<br/>
- `W = tf.Variable(tf.random.uniform([1], -1.0, 1.0))`이 코드는 W를 -1과 1 사이의 균일 분포에서 랜덤하게 생성된 값으로 초기화됩니다.
- `b = tf.Variable(tf.zeros([1]))`이 코드는 b를 0으로 초기화 시키는 코드입니다.
- cost 함수는 손실 함수를 정의합니다. 모델의 예측 값(y)과 실제 값(y_data) 사이의 차이를 제곱하여 평균을 계산합니다.
- optimizer은 경사 하강법을 이용합니다. 손실 함수의 기울기를 계산하고 이 기울기가 감소하는 방향으로 매개변수를 조정합니다.
- 이렇게 하면 손실 함수의 값이 점차 줄어들게 되어 모델의 예측 오차가 줄어들게 됩니다.
- 그 다음 100단계마다 학습단계, 손실 함수의 값, 매개변수의 값을 출력하고 그래프를 보여줍니다.<br/><br/>

<h2>3-3</h2>

![image](https://github.com/ymin1108/ymin1108.github.io/assets/71661158/71ae5627-28c8-4cf9-8200-151f4d2b608e)<br/>
- cost 함수까지는 크게 다른 것이 없으므로 생략합니다.
- update_Weight 함수는 기중치를 업데이트하는 함수입니다.
- `gradient = tf.reduce_mean((W * x_data - y_data) * x_data)`이 부분은 손실 함수의 기울기를 계산합니다.
- `descent = W - learning_rate * gradient` 이 부분은 현재의 가중치에서 학습률과 기울기의 곱을 뺀 값을 새로운 가중치로 설정합니다.
- `update = W.assign(descent)` 이 부분은 새로 계산된 가중치를 현재의 가중치에 할당합니다.<br/><br/>

<h2>3-4</h2>

![image](https://github.com/ymin1108/ymin1108.github.io/assets/71661158/d782ff35-2a00-46ca-b1d3-44904a6dca49)<br/>
- `W = tf.Variable(tf.random.normal([1]), name='weight')` 이 부분까지도 위에서 설명한 적이 있으므로 생략하겠습니다.
- for문에서 가중치는 -3.0부터 5.0까지 0.1씩 변화됩니다.그리고 각 가중치에 대한 손실 함수의 값은 `curr_cost = tf.reduce_mean(tf.square(hypothesis - Y))`를 통해 계산됩니다.
- `W_history`는 가중치 값을, `curr_cost`는 손실 함수의 값을 저장합니다.

<h2>3-5</h2>

![image](https://github.com/ymin1108/ymin1108.github.io/assets/71661158/32fdaf86-18ba-431d-999c-251efbd379da)<br/>
![image](https://github.com/ymin1108/ymin1108.github.io/assets/71661158/1f7e7b87-8d83-4fc8-b18a-344a2f671f20)<br/>
- 이 코드도 경사 하강법을 이용합니다.
- for문에서 학습 단계, 이전 가중치, 현재 가중치를 출력합니다.
- 만약 현재 가중치가 이전 가중치와 같다면 반복문을 멈춥니다.
- 결과를 확인해보면 138번의 학습 후에 가중치가 더 이상 변하지 않는 것을 확인할 수 있습니다.
- 이는 모델이 잘 학습했다라는 것을 의미합니다.
